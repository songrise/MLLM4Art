# ðŸŽ¨ Multimodal LLMs Can Reason about Aesthetics in Zero-Shot | [Paper](https://dl.acm.org/doi/10.1145/3746027.3754961) |[Arxiv](https://arxiv.org/abs/2501.09012) 
TL;DR :book:: How can AI models be truly aligned with the multi-faceted human aesthetic judgment? This paper points out that zero-shot reasoning with MLLMs as a promising solution. Extensive experiment reveals that zero-shot aesthetic reasoning outperform the SOTA image aesthetic assessment models, even without any training. 
![fig_teaser](asset/fig_teaser.png)
---

## Update
:fire::fire:: Jul 05: This work is accepted to ACM MM 2025!

## The FineArtBench Dataset 

![fig_dataset](asset/fig_dataset.jpg)

The 1,000 content, 1,000 style, and stylized images from 10 models can be downloaded from the links below:
**Download Link**: [ç™¾åº¦ç½‘ç›˜ (Baidu NetDisk)](https://pan.baidu.com/s/1jx4qFMHupZHTwyte6fIUsg?pwd=m481) | [Google Drive](https://drive.google.com/file/d/11Cqfz11TuVB13l6wpVefVEY2_-sfrHYB/view?usp=drive_link)


<details>
<summary><h2>Benchmark Guide</h2></summary>

### Dataset Preparation
Please download and extract the dataset and put it under `data/` folder of the root directory. The directory structure should be as follows:
```
MLLM4Art/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 2AFC/               #the pair-wise comparison tasks
â”‚   â”œâ”€â”€ base/               #the content, style images and their annotations
â”‚   â”œâ”€â”€ human_annotation/   #the human aesthetic judgment for 2AFC tasks
â”‚   â”œâ”€â”€ painting/           # the paintings generated by different models
```
### Benchmark Custom Models
To test your own local model on the 2AFC tasks of the FineArtBench, please refer to `custom_model_evaluate.py`. Specifically, you shall implement the `Evaluator` interface with your own model inference logic and map the model prediction to pairwise judgment.  Then you can run the benchmark with the following command:

```bash
python custom_model_evaluate.py 
```

To test API-based MLLMs, please refer to `mllm_API_evaluate.py`. You need to set up your own API key and base URL in the script. The exemplar config can be found in `./APIConfig/`. Then you can run the benchmark with the following command:
```bash
python mllm_API_evaluate.py --config <REPLACE_WITH_YOUR_CONFIG_PATH>
```

The expected output is a JSON file in the same format as `./data/2AFC/2AFC_global_N_5000.json`, but with the `winner` field filled in according to your model's predictions.

### Benchmark Correlation Performance

Once you have the model predictions in the required JSON format, you can evaluate the correlation performance with human judgments using `benchmark.py`. This is the script that gives you the quantitative scores (correlation and statistical significance). You can run the benchmark with the following command:

```bash
python benchmark.py --human_annotation <PATH_TO_HUMAN_ANNOTATION_JSON> --model_annotation <PATH_TO_YOUR_MODEL_PREDICTION_JSON> --mode <global/instance>
```

The `--human_annotation` annotation can be found in `./data/human_annotation/` folder. The `mode` argument specifies whether to compute global (per-artist) correlation or instance-level (per-task) correlation, which corresponds to the two columns in Table 1 of our [paper](https://dl.acm.org/doi/10.1145/3746027.3754961).

The annotation files without `_paper` suffix are recommended, which contains 40% more annotations and with additional quality control. For exactly reproducing the results in our paper, please use the files with `_paper` suffix.

</details>

--- 

## ArtCoT

We propose ArtCoT to enhance the inference-time reasoning capability of MLLMs. An example conversation is provided below. Detailed quantitative comparison can be found in [paper](https://dl.acm.org/doi/10.1145/3746027.3754961). The full response from MLLMs in our experiments will also be released to facilitate further research.


![fig_example_style](asset/fig_example_style.jpg)